{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Rv8qUeR7Oz",
        "outputId": "af2033c5-e8f0-4d58-f181-a4d3929be796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Cleaned Player Lines\n",
            "0           so shaken as we are so wan with care\n",
            "1      find we a time for frighted peace to pant\n",
            "2  and breathe shortwinded accents of new broils\n",
            "3         to be commenced in strands afar remote\n",
            "4      no more the thirsty entrance of this soil\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Load the dataset (assuming it's named 'Shakespeare_data.csv')\n",
        "data = pd.read_csv('Shakespeare_data.csv')\n",
        "\n",
        "# Extract the PlayerLine column\n",
        "player_lines = data['PlayerLine']\n",
        "\n",
        "# Filter out lines containing 'ACT', 'SCENE', and stage directions like 'Enter', 'Exit'\n",
        "def filter_dialogue(line):\n",
        "    line = str(line).lower()\n",
        "    if any(keyword in line for keyword in ['act', 'scene', 'enter', 'exit']):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Apply the filter\n",
        "filtered_lines = player_lines.dropna()\n",
        "filtered_lines = filtered_lines[filtered_lines.apply(filter_dialogue)]\n",
        "\n",
        "# Convert to lowercase\n",
        "filtered_lines = filtered_lines.str.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "filtered_lines = filtered_lines.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Remove extra spaces\n",
        "filtered_lines = filtered_lines.apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "# Remove numbers\n",
        "filtered_lines = filtered_lines.apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "# Optional: Remove special characters (if any)\n",
        "filtered_lines = filtered_lines.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Convert to list for further processing\n",
        "cleaned_lines = filtered_lines.tolist()\n",
        "\n",
        "# Create a DataFrame from the cleaned lines\n",
        "cleaned_df = pd.DataFrame(cleaned_lines, columns=['Cleaned Player Lines'])\n",
        "\n",
        "# Display the cleaned DataFrame in table format\n",
        "print(cleaned_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size_limit = 10000  # Limit vocabulary size to 10,000 most frequent words\n",
        "\n",
        "# Initialize the tokenizer with the limited vocab size\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit)\n",
        "\n",
        "# Fit the tokenizer on the cleaned text\n",
        "tokenizer.fit_on_texts(cleaned_lines)\n",
        "\n",
        "# Convert the text to sequences of word indices\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_lines)\n",
        "\n",
        "# Define the reduced vocabulary size\n",
        "vocab_size = min(vocab_size_limit, len(tokenizer.word_index)) + 1\n",
        "print(f\"Reduced Vocabulary size: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqP8EvVnSkSs",
        "outputId": "569a5bff-1ddc-4faa-e883-1fbb0d2dc0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced Vocabulary size: 10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the desired sequence length\n",
        "sequence_length = 5\n",
        "\n",
        "# Create input sequences and corresponding outputs\n",
        "input_sequences = []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        n_gram_sequence = seq[max(i - sequence_length, 0):i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split into X (input) and y (output)\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "\n",
        "# Convert y to one-hot encoding\n",
        "y = np.eye(vocab_size)[y]\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-tyo77fSrkG",
        "outputId": "877e7d61-c62c-4c84-fd9a-6f472eb18c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (656691, 5)\n",
            "Output shape: (656691, 10001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Build the improved LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Increase embedding dimension to 200\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=200, input_length=max_sequence_len - 1))\n",
        "\n",
        "# Add multiple LSTM layers and increase the number of units\n",
        "model.add(LSTM(256, return_sequences=True))  # First LSTM layer with 256 units\n",
        "model.add(Dropout(0.3))  # Increase dropout rate to avoid overfitting\n",
        "\n",
        "model.add(LSTM(256, return_sequences=False))  # Second LSTM layer with 256 units\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add Dense output layer with softmax activation\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model summary to verify the layers\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po9OjaMzS0AI",
        "outputId": "27366e1e-85e3-49c2-c64d-1065a2580f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 200)            2000200   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 256)            467968    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 256)            0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10001)             2570257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5563737 (21.22 MB)\n",
            "Trainable params: 5563737 (21.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define early stopping and model checkpoint callbacks\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True, mode='min', verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train with smaller batch size (e.g., 16) to save memory\n",
        "history = model.fit(X, y, epochs=10, batch_size=16, callbacks=[checkpoint, early_stopping], verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zSPK2I9S7rf",
        "outputId": "5daeb54f-84b5-4345-86d7-61bb40d1b102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "41044/41044 [==============================] - ETA: 0s - loss: 6.2749 - accuracy: 0.0798\n",
            "Epoch 1: loss improved from inf to 6.27488, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 1037s 25ms/step - loss: 6.2749 - accuracy: 0.0798\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41042/41044 [============================>.] - ETA: 0s - loss: 5.9502 - accuracy: 0.1056\n",
            "Epoch 2: loss improved from 6.27488 to 5.95022, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 991s 24ms/step - loss: 5.9502 - accuracy: 0.1056\n",
            "Epoch 3/10\n",
            "41044/41044 [==============================] - ETA: 0s - loss: 5.8573 - accuracy: 0.1131\n",
            "Epoch 3: loss improved from 5.95022 to 5.85731, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 1001s 24ms/step - loss: 5.8573 - accuracy: 0.1131\n",
            "Epoch 4/10\n",
            "41042/41044 [============================>.] - ETA: 0s - loss: 5.8061 - accuracy: 0.1188\n",
            "Epoch 4: loss improved from 5.85731 to 5.80612, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 990s 24ms/step - loss: 5.8061 - accuracy: 0.1188\n",
            "Epoch 5/10\n",
            "41043/41044 [============================>.] - ETA: 0s - loss: 5.7792 - accuracy: 0.1226\n",
            "Epoch 5: loss improved from 5.80612 to 5.77921, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 982s 24ms/step - loss: 5.7792 - accuracy: 0.1226\n",
            "Epoch 6/10\n",
            "41043/41044 [============================>.] - ETA: 0s - loss: 5.7550 - accuracy: 0.1261\n",
            "Epoch 6: loss improved from 5.77921 to 5.75501, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 976s 24ms/step - loss: 5.7550 - accuracy: 0.1261\n",
            "Epoch 7/10\n",
            "41043/41044 [============================>.] - ETA: 0s - loss: 5.7361 - accuracy: 0.1284\n",
            "Epoch 7: loss improved from 5.75501 to 5.73606, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 982s 24ms/step - loss: 5.7361 - accuracy: 0.1284\n",
            "Epoch 8/10\n",
            "41042/41044 [============================>.] - ETA: 0s - loss: 5.7144 - accuracy: 0.1310\n",
            "Epoch 8: loss improved from 5.73606 to 5.71440, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 978s 24ms/step - loss: 5.7144 - accuracy: 0.1310\n",
            "Epoch 9/10\n",
            "41044/41044 [==============================] - ETA: 0s - loss: 5.6972 - accuracy: 0.1334\n",
            "Epoch 9: loss improved from 5.71440 to 5.69716, saving model to best_model.h5\n",
            "41044/41044 [==============================] - 988s 24ms/step - loss: 5.6972 - accuracy: 0.1334\n",
            "Epoch 10/10\n",
            "39258/41044 [===========================>..] - ETA: 42s - loss: 5.6753 - accuracy: 0.1354"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Enable gradient checkpointing to reduce memory usage during backpropagation\n",
        "tf.config.experimental.enable_tensor_float_32_execution(True)\n"
      ],
      "metadata": {
        "id": "dVjGOexLff8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Clear the Keras session\n",
        "K.clear_session()"
      ],
      "metadata": {
        "id": "ZvhAy6iyTgJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q psutil\n",
        "import psutil\n",
        "\n",
        "def print_memory_usage():\n",
        "    print(f\"Used Memory: {psutil.virtual_memory().used / (1024 ** 2):.2f} MB\")\n",
        "    print(f\"Available Memory: {psutil.virtual_memory().available / (1024 ** 2):.2f} MB\")\n",
        "\n",
        "print_memory_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Id149svTtSj",
        "outputId": "6f684f88-0db7-4a72-84e9-3a510f03ccb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Memory: 2781.14 MB\n",
            "Available Memory: 337834.18 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True)\n",
        "\n",
        "# Train the model with the checkpoint\n",
        "history = model.fit(X, y, epochs=20, batch_size=16, verbose=1, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "id": "qQVrauWgUFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8da21I-pTCwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}